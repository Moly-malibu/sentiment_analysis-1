{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on how to load and use the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/javier/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/javier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/javier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from process_text import clean_up_text\n",
    "from process_text import string_cohesion\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2442 X 1963 document-term matrix\n",
      "number of terms:  1963\n",
      "Dictionary:  6105\n",
      "A:  (6105, 1963)\n",
      "0\n",
      "Horrible trash [-1, -1, -1] [-0.8748386165266397, -0.555848299620592, -1.0]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------\n",
    "# Import the custom stop words\n",
    "custom_stop_words = []\n",
    "with open( \"stopwords.txt\", \"r\" ) as fin:\n",
    "    for line in fin.readlines():\n",
    "        custom_stop_words.append( line.strip() )\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "# Import the vocabulary and generate the vectorizer tranformer\n",
    "#--------------------------------------------------------------\n",
    "(A,terms,dict_sample) = joblib.load( \"/home/javier/Documents/sentiment_analysis/sentiment_model/articles-raw.pkl\" )\n",
    "print( \"Loaded %d X %d document-term matrix\" % (A.shape[0], A.shape[1]) )\n",
    "print('number of terms: ',len(terms))#print(dic_sample[0:10])\n",
    "print('Dictionary: ',len(dict_sample))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = custom_stop_words,min_df = 20)\n",
    "A = vectorizer.fit_transform(dict_sample)\n",
    "print('A: ', A.shape)\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "# We load the trained models\n",
    "#--------------------------------------------------------------\n",
    "# The names of the files containing the weights of the model\n",
    "filename1 = 'lr_sentiment_model.sav'\n",
    "filename2 = 'nb_sentiment_model.sav'\n",
    "\n",
    "# Now we load in the trained models\n",
    "loaded_lr = pickle.load(open(filename1, 'rb'))\n",
    "loaded_nb = pickle.load(open(filename2, 'rb'))\n",
    "\n",
    "sample_text = 'Horrible trash'\n",
    "s = str(clean_up_text(sample_text))\n",
    "\n",
    "# Transform the text\n",
    "X0 = vectorizer.transform([s])\n",
    "\n",
    "# Predict the class \n",
    "preds_nb = loaded_nb.predict(X0)\n",
    "preds_lr = loaded_lr.predict(X0)\n",
    "preds_blob =  TextBlob(sample_text).sentiment.polarity\n",
    "if(preds_blob>=0.0):\n",
    "    preds_blob = +1\n",
    "else:\n",
    "    preds_blob = -1\n",
    "\n",
    "\n",
    "\n",
    "# Compute the probability of the class\n",
    "indx_nb= int((preds_nb[0]+1)/2)\n",
    "indx_lr= int((preds_lr[0]+1)/2)\n",
    "prob_nb = (loaded_nb.predict_proba(X0)[0][indx_nb]-.50)*2.0*int(preds_nb[0])\n",
    "prob_lr = (loaded_lr.predict_proba(X0)[0][indx_lr]-0.50)*2.0*int(preds_lr[0])\n",
    "prob_blob = TextBlob(sample_text).sentiment.polarity # [-1,1]\n",
    "\n",
    "print(indx_nb)\n",
    "print(sample_text,[preds_nb[0],preds_lr[0],preds_blob],[prob_nb,prob_lr,prob_blob ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example strings for computing topic cohesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.653513353317976\n",
      "1.0\n",
      "0.5155545175075531\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Example use of the string cohesion metric\n",
    "s1= 'cat dogs birds rabbit'\n",
    "s2= 'chair chair chair chair'\n",
    "s3= 'cars dogs birds rabbit'\n",
    "s4= ''\n",
    "print(string_cohesion(s1))\n",
    "print(string_cohesion(s2))\n",
    "print(string_cohesion(s3))\n",
    "print(string_cohesion(s4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
