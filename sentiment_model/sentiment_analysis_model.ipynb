{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Jupyter notebook will train a sentiment analysis model using BOW-method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/anaconda3/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/javier/anaconda3/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/javier/anaconda3/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/javier/anaconda3/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/javier/anaconda3/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Load in all of the required modules at the beginning\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 44/10000 [00:00<00:22, 433.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Yelp reviews:  10000\n",
      "Total Amazon automotive reviews:  20473\n",
      "Total Amazon office reviews:  53258\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:33<00:00, 298.37it/s]\n",
      "100%|██████████| 10000/10000 [00:57<00:00, 175.41it/s]\n",
      "100%|██████████| 10000/10000 [01:27<00:00, 114.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw reviews: \n",
      "1-Star reviews: 1221\n",
      "2-Star reviews: 1428\n",
      "3-Star reviews: 2768\n",
      "4-Star reviews: 7528\n",
      "5-Star reviews: 17055\n",
      "\n",
      "\n",
      "\n",
      "Balaced reviews: \n",
      "1-Star reviews: 1221\n",
      "2-Star reviews: 1221\n",
      "3-Star reviews: 1221\n",
      "4-Star reviews: 1221\n",
      "5-Star reviews: 1221\n",
      "\n",
      "\n",
      "The total number of reviews:  6105\n"
     ]
    }
   ],
   "source": [
    "# Read in the Yelp review data\n",
    "yelp = pd.read_csv('yelp.csv')\n",
    "yelp = yelp.sample(frac=1)\n",
    "\n",
    "# Read in Amazon review data\n",
    "data_automotive = pd.read_json('reviews_Automotive_5.json', lines=True)\n",
    "data_automotive = data_automotive.sample(frac=1)\n",
    "\n",
    "data_office_products = pd.read_json('reviews_Office_Products_5.json', lines=True)\n",
    "data_office_products  = data_office_products.sample(frac=1)\n",
    "\n",
    "#data_home_kitchen = pd.read_json('reviews_Home_and_Kitchen_5.json',lines=True)\n",
    "#data_home_kitchen = data_home_kitchen.sample(frac=1)\n",
    "\n",
    "# Create a new dataframe which will be used to aggregate all reviews\n",
    "columns = ['review','rating']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# These are the parameters of the model we will train\n",
    "#-------------------------------------------------------------------\n",
    "n_features = 1000 # The number of features to use for constructing the dictionary (size of the vocabulary)\n",
    "n_reviews = 10000 # The number of reviews to use from each data sets\n",
    "train_size = 0.8 # Splits data into two groups which will be further divided\n",
    "test_size = 0.8 # Splits the remaining set into two groups\n",
    "random_state = 10 # For reproducibility \n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------')\n",
    "print('Total Yelp reviews: ',len(yelp))\n",
    "print('Total Amazon automotive reviews: ',len(data_automotive))\n",
    "print('Total Amazon office reviews: ',len(data_office_products))\n",
    "#print('Total Amazon home and kitchen reviews: ',len(data_home_kitchen))\n",
    "print('---------------------------------------------')\n",
    "\n",
    "# Here we read in the reviews from all of the data sets and aggregate them into one data frame\n",
    "for i in tqdm(range(0,min(n_reviews,len(yelp)))):\n",
    "    df=df.append({\"review\":yelp[\"text\"][i],\"rating\":yelp[\"stars\"][i]},ignore_index=True)\n",
    "\n",
    "for i in tqdm(range(0,min(n_reviews,len(data_automotive)))):\n",
    "    df=df.append({\"review\":data_automotive['reviewText'][i],\"rating\": int(data_automotive[\"overall\"][i])},ignore_index=True)\n",
    "\n",
    "for i in tqdm(range(0,min(n_reviews,len(data_office_products)))):\n",
    "    df=df.append({\"review\":data_office_products['reviewText'][i],\"rating\": int(data_office_products[\"overall\"][i])},ignore_index=True)\n",
    "\n",
    "#for i in tqdm(range(0,min(n_reviews,len(data_home_kitchen)))):\n",
    "#    df=df.append({\"review\":data_home_kitchen['reviewText'][i],\"rating\": int(data_home_kitchen[\"overall\"][i])},ignore_index=True)    \n",
    "    \n",
    "# Find the specific star rated reviews\n",
    "df_1_star = df[df[\"rating\"]== 1]\n",
    "df_2_star = df[df[\"rating\"]== 2]\n",
    "df_3_star = df[df[\"rating\"]== 3]\n",
    "df_4_star = df[df[\"rating\"]== 4]\n",
    "df_5_star = df[df[\"rating\"]== 5]\n",
    "\n",
    "print(\"\")\n",
    "print('Raw reviews: ')\n",
    "print(\"1-Star reviews:\",len(df_1_star))\n",
    "print(\"2-Star reviews:\",len(df_2_star))\n",
    "print(\"3-Star reviews:\",len(df_3_star))\n",
    "print(\"4-Star reviews:\",len(df_4_star))\n",
    "print(\"5-Star reviews:\",len(df_5_star))\n",
    "print('\\n')\n",
    "\n",
    "# Aggregate all of the review data\n",
    "min_stars= min(df_1_star.count()[0],df_2_star.count()[0],df_3_star.count()[0],df_4_star.count()[0],df_5_star.count()[0])\n",
    "\n",
    "# Randomize the data set\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "df_1_star = df[df[\"rating\"]== 1].head(min_stars)\n",
    "df_2_star = df[df[\"rating\"]== 2].head(min_stars)\n",
    "df_3_star = df[df[\"rating\"]== 3].head(min_stars)\n",
    "df_4_star = df[df[\"rating\"]== 4].head(min_stars)\n",
    "df_5_star = df[df[\"rating\"]== 5].head(min_stars)\n",
    "\n",
    "print(\"\")\n",
    "print('Balaced reviews: ')\n",
    "print(\"1-Star reviews:\",len(df_1_star))\n",
    "print(\"2-Star reviews:\",len(df_2_star))\n",
    "print(\"3-Star reviews:\",len(df_3_star))\n",
    "print(\"4-Star reviews:\",len(df_4_star))\n",
    "print(\"5-Star reviews:\",len(df_5_star))\n",
    "print('\\n')\n",
    "\n",
    "# Combine all of the dataframes\n",
    "frames = [df_1_star,df_2_star,df_3_star,df_4_star,df_5_star]\n",
    "\n",
    "df_balanced = pd.concat(frames)\n",
    "\n",
    "# Shuffle the data frame to randomize everything\n",
    "df_balanced = df_balanced.sample(frac=1)\n",
    "df_balanced.index = range(0,df_balanced.shape[0]) # Relabel the indices\n",
    "df_balanced.head()\n",
    "\n",
    "print('The total number of reviews: ', len(df_balanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6105 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/javier/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/javier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/javier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6105/6105 [00:35<00:00, 172.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now we need to process the text. We define here a function that will remove the punctuation and stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import operator\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter \n",
    "\n",
    "# In case we dont have the nltk stopwords documents\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "@np.vectorize\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    words = filter(lambda x: x not in stopwords.words('english'), tokens)\n",
    "    # Remove punctuation\n",
    "    #words = re.sub(r'[^\\w\\s]','',words)\n",
    "    #words = [char for char in text if char not in string.punctuation]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def get_pos( word ):\n",
    "    '''\n",
    "    Part-Of-Speech Tagger\n",
    "    '''\n",
    "    w_synsets = wordnet.synsets(word)\n",
    "    \n",
    "    \n",
    "    # n-noun, v-verb, a-adjective, r-\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "    \n",
    "    most_common_pos_list = pos_counts.most_common(3)\n",
    "    return most_common_pos_list[0][0]\n",
    "\n",
    "def clean_up_text(text):\n",
    "    '''\n",
    "    This function will preprocess a text string\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Remove @ from the string\n",
    "    s=re.sub(r'@[A-Za-z0-9]+','',text)    \n",
    "    \n",
    "    # Remove URLS\n",
    "    s=re.sub('https?://[A-Za-z0-9./]+','',s)\n",
    "    \n",
    "    # Set to lower case\n",
    "    s=s.lower()\n",
    "    \n",
    "    # Remove numbers\n",
    "    s= re.sub(r'[0-9]+', '', s) \n",
    "    \n",
    "    # Remove punctuation\n",
    "    s = re.sub(r'[^\\w\\s]','',s)\n",
    "    \n",
    "    # Remove underscore\n",
    "    s=s.replace(\"_\", \" \")\n",
    "    \n",
    "    # Remove RT from the tweet\n",
    "    s=s.replace(\"rt\", \"\")\n",
    "    \n",
    "    # Tokenize the words\n",
    "    words = word_tokenize(s)\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    s_stem = ''\n",
    "    for wi in words:\n",
    "        #wi = ps.stem(wi)\n",
    "        wi = wnl.lemmatize(wi,get_pos(wi))\n",
    "        s_stem += ' ' + wi\n",
    "    \n",
    "    return s_stem\n",
    "\n",
    "\n",
    "# Let us now generate a subset of the data to create a vocabulary with\n",
    "n_sample = len(df_balanced)\n",
    "dict_sample = []\n",
    "\n",
    "# Now we preprocess the entire balanced data set\n",
    "for i in tqdm(range(0,n_sample)):\n",
    "    sentence = df_balanced[\"review\"][i]\n",
    "    dict_sample.append(clean_up_text(sentence))\n",
    "\n",
    "# Convert to numpy array\n",
    "dict_sample = np.asarray(dict_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6105 X 1962 TF-IDF-normalized document-term matrix\n",
      "\n",
      "01. not (260.13)\n",
      "02. get (198.64)\n",
      "03. good (190.01)\n",
      "04. go (174.98)\n",
      "05. like (171.94)\n",
      "06. place (168.99)\n",
      "07. food (165.27)\n",
      "08. great (147.65)\n",
      "09. work (146.06)\n",
      "10. time (141.24)\n",
      "11. can (124.37)\n",
      "12. no (120.00)\n",
      "13. dont (115.10)\n",
      "14. order (115.09)\n",
      "15. really (112.33)\n",
      "16. price (111.28)\n",
      "17. buy (105.67)\n",
      "18. look (104.36)\n",
      "19. try (103.37)\n",
      "20. need (103.13)\n"
     ]
    }
   ],
   "source": [
    "# we can pass in the same preprocessing parameters\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "custom_stop_words = []\n",
    "with open( \"stopwords.txt\", \"r\" ) as fin:\n",
    "    for line in fin.readlines():\n",
    "        custom_stop_words.append( line.strip() )\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# Here we vectorize the dictionary sample\n",
    "vectorizer = TfidfVectorizer(stop_words = custom_stop_words,min_df = 20)\n",
    "A = vectorizer.fit_transform(dict_sample)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (A.shape[0], A.shape[1]) )\n",
    "print(\"\")\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# extract the resulting vocabulary\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "def rank_terms( A, terms ):\n",
    "    # get the sums over each column\n",
    "    sums = A.sum(axis=0)\n",
    "    # map weights to the terms\n",
    "    weights = {}\n",
    "    for col, term in enumerate(terms):\n",
    "        weights[term] = sums[0,col]\n",
    "    # rank the terms by their weight over all documents\n",
    "    return sorted(weights.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "ranking = rank_terms( A, terms )\n",
    "for i, pair in enumerate( ranking[0:20] ):\n",
    "    print( \"%02d. %s (%.2f)\" % ( i+1, pair[0], pair[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6105/6105 [00:00<00:00, 53613.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Dataset:  2442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X- Feature matrix:  (2442, 1962)\n",
      "X_train:  1953\n",
      "X_test:  391\n",
      "X_validation:  98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Now we must transform our original review data into a feature Matrix\n",
    "y = df_balanced[\"rating\"]\n",
    "\n",
    "# Now we must transform our original review data into a feature Matrix\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# converting the data frame into a feature matrix\n",
    "for i in tqdm(range(0,df_balanced.shape[0])):\n",
    "    \n",
    "    r = df_balanced[\"rating\"][i]\n",
    "    \n",
    "    if(r==1):\n",
    "        yi =-1\n",
    "        X.append(df_balanced[\"review\"][i])\n",
    "        y.append(yi)\n",
    "    elif(r==5):\n",
    "        yi=1\n",
    "        X.append(df_balanced[\"review\"][i])\n",
    "        y.append(yi)\n",
    "print('Balanced Dataset: ',len(y))        \n",
    "\n",
    "## We store the remaining data \n",
    "df_final = df_balanced.drop(df_balanced[(df_balanced.rating == 2) | (df_balanced.rating == 3)| (df_balanced.rating== 4)].index)\n",
    "\n",
    "\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "# Converts the Document matrix consisting of strings into arrays according to the dictionary that we built previously\n",
    "X= vectorizer.transform(X)\n",
    "\n",
    "print('X- Feature matrix: ', X.shape)\n",
    "\n",
    "# Now we split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, random_state=random_state)\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, train_size=test_size, random_state=random_state)\n",
    "\n",
    "print('X_train: ', len(y_train))\n",
    "print('X_test: ', len(y_test))\n",
    "print('X_validation: ', len(y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "\n",
      "Train data set size:  1953 \n",
      "\n",
      "Test data set size:  391 \n",
      "\n",
      "================================================================\n",
      "\n",
      "Baseline Model: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.52      0.54      0.53       196\n",
      "          1       0.52      0.49      0.50       195\n",
      "\n",
      "avg / total       0.52      0.52      0.52       391\n",
      "\n",
      "================================================================\n",
      "\n",
      "Naive Bayes: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.82      0.74      0.78       196\n",
      "          1       0.76      0.84      0.80       195\n",
      "\n",
      "avg / total       0.79      0.79      0.79       391\n",
      "\n",
      "================================================================\n",
      "\n",
      "Desicion Tree: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.70      0.61      0.65       196\n",
      "          1       0.65      0.73      0.69       195\n",
      "\n",
      "avg / total       0.67      0.67      0.67       391\n",
      "\n",
      "================================================================\n",
      "\n",
      "Random Forests: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.79      0.77      0.78       196\n",
      "          1       0.77      0.79      0.78       195\n",
      "\n",
      "avg / total       0.78      0.78      0.78       391\n",
      "\n",
      "================================================================\n",
      "\n",
      "Logistic Regression: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.81      0.79      0.80       196\n",
      "          1       0.79      0.82      0.80       195\n",
      "\n",
      "avg / total       0.80      0.80      0.80       391\n",
      "\n",
      "================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy.random import randint\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Now we train different models\n",
    "nb = MultinomialNB()\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(max_depth=20, random_state=0)\n",
    "lr = LogisticRegression(multi_class='multinomial',solver='newton-cg')\n",
    "\n",
    "# Fit the different machine learning models\n",
    "nb.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# The baseline model generates (-1 or 1) randomly\n",
    "preds_bl = np.ones(len(y_test))-2*randint(0,2,len(y_test))\n",
    "preds_nb = nb.predict(X_test)\n",
    "preds_dt = dt.predict(X_test)\n",
    "preds_rf = rf.predict(X_test)\n",
    "preds_lr = lr.predict(X_test)\n",
    "preds_lr_val = lr.predict(X_validation)\n",
    "\n",
    "print('================================================================\\n')\n",
    "print('Train data set size: ', len(y_train),'\\n')\n",
    "print('Test data set size: ', len(y_test),'\\n')\n",
    "print('================================================================\\n')\n",
    "print(\"Baseline Model: \\n\",classification_report(y_test,preds_bl))\n",
    "print('================================================================\\n')\n",
    "print(\"Naive Bayes: \\n\" ,classification_report(y_test,preds_nb))\n",
    "print('================================================================\\n')\n",
    "print(\"Desicion Tree: \\n\",classification_report(y_test,preds_dt))\n",
    "print('================================================================\\n')\n",
    "print(\"Random Forests: \\n\",classification_report(y_test,preds_rf))\n",
    "print('================================================================\\n')\n",
    "print(\"Logistic Regression: \\n\",classification_report(y_test,preds_lr))\n",
    "print('================================================================\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['articles-raw.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we save the model\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "filename1 = 'lr_sentiment_model.sav'\n",
    "filename2 = 'nb_sentiment_model.sav'\n",
    "pickle.dump(lr, open(filename1, 'wb'))\n",
    "pickle.dump(nb, open(filename2, 'wb'))\n",
    "joblib.dump((X,terms,dict_sample), \"articles-raw.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model, test the models on some example text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This car does not deserve a rating\n",
      "  (0, 1356)\t0.5556988969708335\n",
      "  (0, 1140)\t0.19816603945631472\n",
      "  (0, 467)\t0.6954515776261395\n",
      "  (0, 256)\t0.410214650992515\n",
      "\n",
      "lr:  [-1] 0.6187717910688091\n",
      "nb:  [-1] 0.6129657703156374\n",
      "TxtBlob:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Load the models and use them to make a prediction\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Import the vocabulary \n",
    "#(A,terms,snippets) = joblib.load( \"/home/javier/Documents/sentiment_analysis/topic_modelling/articles-tfidf.pkl\" )\n",
    "#print( \"Loaded %d X %d document-term matrix\" % (A.shape[0], A.shape[1]) )\n",
    "\n",
    "#vectorizer = TfidfVectorizer(stop_words = custom_stop_words,min_df = 20)\n",
    "#A = vectorizer.fit_transform(dict_sample)\n",
    "\n",
    "# The names of the files containing the weights of the model\n",
    "filename1 = 'lr_sentiment_model.sav'\n",
    "filename2 = 'nb_sentiment_model.sav'\n",
    "\n",
    "# Now we load in the trained models\n",
    "loaded_lr = pickle.load(open(filename1, 'rb'))\n",
    "loaded_nb = pickle.load(open(filename2, 'rb'))\n",
    "\n",
    "sample_text = 'This car does not deserve a rating'\n",
    "s = clean_up_text(sample_text)\n",
    "\n",
    "print(sample_text)#,df_balanced[\"rating\"][312])\n",
    "\n",
    "# Transform the text\n",
    "X0 = vectorizer.transform([s])\n",
    "print(X0)\n",
    "preds_nb = loaded_nb.predict(X0)\n",
    "preds_lr = loaded_lr.predict(X0)\n",
    "\n",
    "prob_nb = np.max(loaded_nb.predict_proba(X0))\n",
    "prob_lr = np.max(loaded_lr.predict_proba(X0))\n",
    "\n",
    "print(\"\")\n",
    "print('lr: ',preds_lr,prob_lr)\n",
    "print('nb: ',preds_nb,prob_nb)\n",
    "print('TxtBlob: ', TextBlob(s).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
