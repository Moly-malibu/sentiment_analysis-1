{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Yelp review data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "10000\n",
      "20473\n",
      "53258\n",
      "---------------------------------------------\n",
      "\n",
      "1-Star reviews: 575\n",
      "2-Star reviews: 575\n",
      "3-Star reviews: 575\n",
      "4-Star reviews: 575\n",
      "5-Star reviews: 575\n",
      "2875\n"
     ]
    }
   ],
   "source": [
    "# Read in the Yelp review data\n",
    "yelp = pd.read_csv('yelp.csv')\n",
    "\n",
    "# Read in Amazon review data\n",
    "data_automotive = pd.read_json('reviews_Automotive_5.json', lines=True)\n",
    "data_office_products = pd.read_json('reviews_Office_Products_5.json', lines=True)\n",
    "\n",
    "# Combine the reviews into [ID,text,review]\n",
    "columns = ['review','rating']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "print('---------------------------------------------')\n",
    "print(len(yelp))\n",
    "print(len(data_automotive))\n",
    "print(len(data_office_products))\n",
    "print('---------------------------------------------')\n",
    "\n",
    "for i in range(0,5000):\n",
    "    df=df.append({\"review\":yelp[\"text\"][i],\"rating\":yelp[\"stars\"][i]},ignore_index=True)\n",
    "\n",
    "for i in range(0,5000):\n",
    "    df=df.append({\"review\":data_automotive['reviewText'][i],\"rating\": int(data_automotive[\"overall\"][i])},ignore_index=True)\n",
    "\n",
    "for i in range(0,5000):\n",
    "    df=df.append({\"review\":data_office_products['reviewText'][i],\"rating\": int(data_office_products[\"overall\"][i])},ignore_index=True)\n",
    "    \n",
    "\n",
    "df_1_star = df[df[\"rating\"]== 1]\n",
    "df_2_star = df[df[\"rating\"]== 2]\n",
    "df_3_star = df[df[\"rating\"]== 3]\n",
    "df_4_star = df[df[\"rating\"]== 4]\n",
    "df_5_star = df[df[\"rating\"]== 5]\n",
    "\n",
    "min_stars= min(df_1_star.count()[0],df_2_star.count()[0],df_3_star.count()[0],df_4_star.count()[0],df_5_star.count()[0])\n",
    "\n",
    "# Randomize the data set\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "df_1_star = df[df[\"rating\"]== 1].head(min_stars)\n",
    "df_2_star = df[df[\"rating\"]== 2].head(min_stars)\n",
    "df_3_star = df[df[\"rating\"]== 3].head(min_stars)\n",
    "df_4_star = df[df[\"rating\"]== 4].head(min_stars)\n",
    "df_5_star = df[df[\"rating\"]== 5].head(min_stars)\n",
    "\n",
    "print(\"\")\n",
    "print(\"1-Star reviews:\",len(df_1_star))\n",
    "print(\"2-Star reviews:\",len(df_2_star))\n",
    "print(\"3-Star reviews:\",len(df_3_star))\n",
    "print(\"4-Star reviews:\",len(df_4_star))\n",
    "print(\"5-Star reviews:\",len(df_5_star))\n",
    "\n",
    "\n",
    "# Combine all of the dataframes\n",
    "frames = [df_1_star,df_2_star,df_3_star,df_4_star,df_5_star]\n",
    "\n",
    "df_balanced = pd.concat(frames)\n",
    "\n",
    "# Shuffle the data frame to randomize everything\n",
    "df_balanced = df_balanced.sample(frac=1)\n",
    "df_balanced.index = range(0,df_balanced.shape[0]) # Relabel the indices\n",
    "df_balanced.head()\n",
    "\n",
    "print(len(df_balanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/javier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2875/2875 [00:53<00:00, 54.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now we need to process the text. We define here a function that will remove the punctuation and stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import operator\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter \n",
    "\n",
    "# In case we dont have the nltk stopwords documents\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "@np.vectorize\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    words = filter(lambda x: x not in stopwords.words('english'), tokens)\n",
    "    # Remove punctuation\n",
    "    #words = re.sub(r'[^\\w\\s]','',words)\n",
    "    #words = [char for char in text if char not in string.punctuation]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_pos( word ):\n",
    "    '''\n",
    "    Part-Of-Speech Tagger\n",
    "    '''\n",
    "    w_synsets = wordnet.synsets(word)\n",
    "    \n",
    "    \n",
    "    # n-noun, v-verb, a-adjective, r-\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "    \n",
    "    most_common_pos_list = pos_counts.most_common(3)\n",
    "    return most_common_pos_list[0][0]\n",
    "\n",
    "def clean_up_tweet(text):\n",
    "    # Remove @ from the string\n",
    "    s=re.sub(r'@[A-Za-z0-9]+','',text)    \n",
    "    \n",
    "    # Remove URLS\n",
    "    s=re.sub('https?://[A-Za-z0-9./]+','',s)\n",
    "    \n",
    "    # Set to lower case\n",
    "    s=s.lower()\n",
    "    \n",
    "    # Remove numbers\n",
    "    s= re.sub(r'[0-9]+', '', s) \n",
    "    \n",
    "    # Remove punctuation\n",
    "    s = re.sub(r'[^\\w\\s]','',s)\n",
    "    \n",
    "    # Remove underscore\n",
    "    s=s.replace(\"_\", \" \")\n",
    "    \n",
    "    # Remove RT from the tweet\n",
    "    s=s.replace(\"rt\", \"\")\n",
    "    \n",
    "    # Tokenize the words\n",
    "    words = word_tokenize(s)\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    s_stem = ''\n",
    "    for wi in words:\n",
    "        #wi = ps.stem(wi)\n",
    "        wi = wnl.lemmatize(wi,get_pos(wi))\n",
    "        s_stem += ' ' + wi\n",
    "    \n",
    "    return s_stem\n",
    "\n",
    "\n",
    "# Let us now generate a subset of the data to create a vocabulary with\n",
    "n_sample = len(df_balanced)\n",
    "n_features = 1000 # Number of features to use for constructing the dictionary\n",
    "dict_sample = []\n",
    "\n",
    "# Now we preprocess the entire balanced data set\n",
    "# yelp_balanced.shape[0]\n",
    "for i in tqdm(range(0,n_sample)):\n",
    "    sentence = df_balanced[\"review\"][i]\n",
    "    dict_sample.append(clean_up_tweet(sentence))\n",
    "\n",
    "    # Convert to numpy array\n",
    "dict_sample = np.asarray(dict_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2875 X 1312 TF-IDF-normalized document-term matrix\n",
      "\n",
      "Vocabulary has 1312 distinct terms\n",
      "01. the (435.55)\n",
      "02. be (382.01)\n",
      "03. and (270.07)\n",
      "04. to (242.90)\n",
      "05. it (223.35)\n",
      "06. of (164.83)\n",
      "07. have (154.40)\n",
      "08. for (148.31)\n",
      "09. this (138.79)\n",
      "10. in (130.07)\n",
      "11. that (122.17)\n",
      "12. my (115.96)\n",
      "13. you (115.57)\n",
      "14. not (111.21)\n",
      "15. but (110.34)\n",
      "16. on (110.30)\n",
      "17. with (104.73)\n",
      "18. they (103.24)\n",
      "19. use (86.29)\n",
      "20. we (80.65)\n"
     ]
    }
   ],
   "source": [
    "# we can pass in the same preprocessing parameters\n",
    "vectorizer = TfidfVectorizer( min_df = 20)\n",
    "A = vectorizer.fit_transform(dict_sample)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (A.shape[0], A.shape[1]) )\n",
    "print(\"\")\n",
    "\n",
    "# extract the resulting vocabulary\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))\n",
    "\n",
    "\n",
    "\n",
    "def rank_terms( A, terms ):\n",
    "    # get the sums over each column\n",
    "    sums = A.sum(axis=0)\n",
    "    # map weights to the terms\n",
    "    weights = {}\n",
    "    for col, term in enumerate(terms):\n",
    "        weights[term] = sums[0,col]\n",
    "    # rank the terms by their weight over all documents\n",
    "    return sorted(weights.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "ranking = rank_terms( A, terms )\n",
    "for i, pair in enumerate( ranking[0:20] ):\n",
    "    print( \"%02d. %s (%.2f)\" % ( i+1, pair[0], pair[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Now we must transform our original review data into a feature Matrix\n",
    "y = df_balanced[\"rating\"]\n",
    "\n",
    "\n",
    "# Now we must transform our original review data into a feature Matrix\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# converting the data frame into a feature matrix\n",
    "for i in range(0,df_balanced.shape[0]):\n",
    "    \n",
    "    r = df_balanced[\"rating\"][i]\n",
    "    \n",
    "    if(r==1 or r==2):\n",
    "        yi =-1\n",
    "        X.append(df_balanced[\"review\"][i])\n",
    "        y.append(yi)\n",
    "    elif(r==4 or r==5):\n",
    "        yi=1\n",
    "        X.append(df_balanced[\"review\"][i])\n",
    "        y.append(yi)\n",
    "\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Now we split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "\n",
      "Baseline Model: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.50      0.49      0.49       928\n",
      "          0       0.00      0.00      0.00         0\n",
      "          1       0.00      0.00      0.00       912\n",
      "\n",
      "avg / total       0.25      0.25      0.25      1840\n",
      "\n",
      "================================================================\n",
      "\n",
      "Naive Bayes: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.80      0.74      0.77       928\n",
      "          1       0.76      0.81      0.79       912\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1840\n",
      "\n",
      "================================================================\n",
      "\n",
      "Desicion Tree: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.63      0.58      0.61       928\n",
      "          1       0.61      0.66      0.63       912\n",
      "\n",
      "avg / total       0.62      0.62      0.62      1840\n",
      "\n",
      "================================================================\n",
      "\n",
      "Random Forests: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.69      0.62      0.65       928\n",
      "          1       0.65      0.71      0.68       912\n",
      "\n",
      "avg / total       0.67      0.67      0.67      1840\n",
      "\n",
      "================================================================\n",
      "\n",
      "Logistic Regression: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.80      0.76      0.78       928\n",
      "          1       0.77      0.81      0.79       912\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1840\n",
      "\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/javier/anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy.random import randint\n",
    "\n",
    "# Now we train different models\n",
    "nb = MultinomialNB()\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(max_depth=20, random_state=0)\n",
    "lr = LogisticRegression(multi_class='multinomial',solver='newton-cg')\n",
    "\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "preds_bl = randint(-1,1,len(y_test))\n",
    "preds_nb = nb.predict(X_test)\n",
    "preds_dt = dt.predict(X_test)\n",
    "preds_rf = rf.predict(X_test)\n",
    "preds_lr = lr.predict(X_test)\n",
    "\n",
    "print('================================================================\\n')\n",
    "print(\"Baseline Model: \\n\",classification_report(y_test,preds_bl))\n",
    "print('================================================================\\n')\n",
    "print(\"Naive Bayes: \\n\" ,classification_report(y_test,preds_nb))\n",
    "print('================================================================\\n')\n",
    "print(\"Desicion Tree: \\n\",classification_report(y_test,preds_dt))\n",
    "print('================================================================\\n')\n",
    "print(\"Random Forests: \\n\",classification_report(y_test,preds_rf))\n",
    "print('================================================================\\n')\n",
    "print(\"Logistic Regression: \\n\",classification_report(y_test,preds_lr))\n",
    "print('================================================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I sure loved it\n",
      " i sure love it\n",
      "\n",
      "[1] [[ 0.29031207  0.70968793]]\n"
     ]
    }
   ],
   "source": [
    "#print(df_balanced[\"review\"][310])\n",
    "#print(vectorizer.transform([df_balanced[\"review\"][0]]))\n",
    "#print(X_train[0])\n",
    "text = \"I sure loved it\"\n",
    "s = clean_up_tweet(text)\n",
    "\n",
    "print(text)\n",
    "print(s)\n",
    "\n",
    "X0 = vectorizer.transform([s])\n",
    "preds_lr = nb.predict(X0)\n",
    "prob_lr = nb.predict_proba(X0)\n",
    "\n",
    "print(\"\")\n",
    "print(preds_lr,prob_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
