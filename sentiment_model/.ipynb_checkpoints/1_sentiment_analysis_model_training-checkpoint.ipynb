{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Jupyter notebook will train a sentiment analysis model using BOW-method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all of the required modules at the beginning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and balance the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/10000 [00:00<01:03, 156.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Yelp reviews:  10000\n",
      "Total Amazon automotive reviews:  20473\n",
      "Total Amazon office reviews:  53258\n",
      "Total Twitter Data reviews:  70000\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:20<00:00, 124.16it/s]\n",
      "100%|██████████| 70000/70000 [13:45<00:00, 84.81it/s] \n",
      "100%|██████████| 20473/20473 [16:51<00:00, 20.24it/s]\n",
      "100%|██████████| 53258/53258 [1:05:42<00:00, 13.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw reviews: \n",
      "1-Star reviews: 2421\n",
      "2-Star reviews: 3259\n",
      "3-Star reviews: 7951\n",
      "4-Star reviews: 57340\n",
      "5-Star reviews: 47592\n",
      "\n",
      "\n",
      "\n",
      "Balaced reviews: \n",
      "1-Star reviews: 2421\n",
      "2-Star reviews: 2421\n",
      "3-Star reviews: 2421\n",
      "4-Star reviews: 2421\n",
      "5-Star reviews: 2421\n",
      "\n",
      "\n",
      "The total number of reviews:  12105\n"
     ]
    }
   ],
   "source": [
    "# Read in the Yelp review data\n",
    "yelp = pd.read_csv('model_training_data/yelp.csv')\n",
    "yelp = yelp.sample(frac=1)\n",
    "\n",
    "# Read in Amazon review data\n",
    "data_automotive = pd.read_json('model_training_data/reviews_Automotive_5.json', lines=True)\n",
    "data_automotive = data_automotive.sample(frac=1)\n",
    "\n",
    "data_office_products = pd.read_json('model_training_data/reviews_Office_Products_5.json', lines=True)\n",
    "data_office_products  = data_office_products.sample(frac=1)\n",
    "\n",
    "\n",
    "#=====================================================================================================\n",
    "# Here we read in the twitter sentiment data and map it to our prefered labels\n",
    "#=====================================================================================================\n",
    "\n",
    "def func(true_sentiment):\n",
    "    \n",
    "    y_true = None\n",
    "    \n",
    "    if(true_sentiment==0):\n",
    "        y_true = 1\n",
    "    elif(true_sentiment==4):\n",
    "        y_true = 5\n",
    "    \n",
    "    return y_true\n",
    "\n",
    "# Specify the name of the file, along with the name of the columns\n",
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "\n",
    "# Read in the data, randomly shuffle and reset the index\n",
    "data_twitter=pd.read_csv('model_training_data/training.1600000.processed.noemoticon.csv',header=None, names=cols,encoding = \"ISO-8859-1\")\n",
    "data_twitter=data_twitter.sample(frac=1)\n",
    "data_twitter=data_twitter.reset_index(drop=True)\n",
    "\n",
    "# Take a subset of the data for faster evaluation\n",
    "data_twitter = data_twitter[0:70000]\n",
    "data_twitter[\"text\"]=data_twitter[\"text\"].apply(func)\n",
    "\n",
    "#=====================================================================================================\n",
    "\n",
    "#data_home_kitchen = pd.read_json('reviews_Home_and_Kitchen_5.json',lines=True)\n",
    "#data_home_kitchen = data_home_kitchen.sample(frac=1)\n",
    "\n",
    "# Create a new dataframe which will be used to aggregate all reviews\n",
    "columns = ['review','rating']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# These are the parameters of the model we will train\n",
    "#-------------------------------------------------------------------\n",
    "n_reviews = 70000 # The number of reviews to use from each data sets\n",
    "train_size = 0.8 # Splits data into two groups which will be further divided\n",
    "random_state = 10 # For reproducibility \n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------')\n",
    "print('Total Yelp reviews: ',len(yelp))\n",
    "print('Total Amazon automotive reviews: ',len(data_automotive))\n",
    "print('Total Amazon office reviews: ',len(data_office_products))\n",
    "print(\"Total Twitter Data reviews: \", len(data_twitter))\n",
    "#print('Total Amazon home and kitchen reviews: ',len(data_home_kitchen))\n",
    "print('---------------------------------------------')\n",
    "\n",
    "# Here we read in the reviews from all of the data sets and aggregate them into one data frame\n",
    "for i in tqdm(range(0,min(n_reviews,len(yelp)))):\n",
    "    df=df.append({\"review\":yelp[\"text\"][i],\"rating\":yelp[\"stars\"][i]},ignore_index=True)\n",
    "    \n",
    "for i in tqdm(range(0,min(n_reviews,len(data_twitter)))):\n",
    "    df=df.append({\"review\":data_twitter[\"text\"][i],\"rating\":data_twitter[\"sentiment\"][i]},ignore_index=True)\n",
    "\n",
    "for i in tqdm(range(0,min(n_reviews,len(data_automotive)))):\n",
    "    df=df.append({\"review\":data_automotive['reviewText'][i],\"rating\": int(data_automotive[\"overall\"][i])},ignore_index=True)\n",
    "\n",
    "for i in tqdm(range(0,min(n_reviews,len(data_office_products)))):\n",
    "    df=df.append({\"review\":data_office_products['reviewText'][i],\"rating\": int(data_office_products[\"overall\"][i])},ignore_index=True)\n",
    "\n",
    "#for i in tqdm(range(0,min(n_reviews,len(data_home_kitchen)))):\n",
    "#    df=df.append({\"review\":data_home_kitchen['reviewText'][i],\"rating\": int(data_home_kitchen[\"overall\"][i])},ignore_index=True)    \n",
    "    \n",
    "# Find the specific star rated reviews\n",
    "df_1_star = df[df[\"rating\"]== 1]\n",
    "df_2_star = df[df[\"rating\"]== 2]\n",
    "df_3_star = df[df[\"rating\"]== 3]\n",
    "df_4_star = df[df[\"rating\"]== 4]\n",
    "df_5_star = df[df[\"rating\"]== 5]\n",
    "\n",
    "print(\"\")\n",
    "print('Raw reviews: ')\n",
    "print(\"1-Star reviews:\",len(df_1_star))\n",
    "print(\"2-Star reviews:\",len(df_2_star))\n",
    "print(\"3-Star reviews:\",len(df_3_star))\n",
    "print(\"4-Star reviews:\",len(df_4_star))\n",
    "print(\"5-Star reviews:\",len(df_5_star))\n",
    "print('\\n')\n",
    "\n",
    "# Aggregate all of the review data\n",
    "# find the minimum of the extreme star reviews which will be used for training\n",
    "#min_stars= min(df_1_star.count()[0],df_2_star.count()[0],df_3_star.count()[0],df_4_star.count()[0],df_5_star.count()[0])\n",
    "min_stars= min(df_1_star.count()[0],df_5_star.count()[0])\n",
    "\n",
    "\n",
    "# Randomize the data set\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "df_1_star = df[df[\"rating\"]== 1].head(min_stars)\n",
    "df_2_star = df[df[\"rating\"]== 2].head(min_stars)\n",
    "df_3_star = df[df[\"rating\"]== 3].head(min_stars)\n",
    "df_4_star = df[df[\"rating\"]== 4].head(min_stars)\n",
    "df_5_star = df[df[\"rating\"]== 5].head(min_stars)\n",
    "\n",
    "print(\"\")\n",
    "print('Balaced reviews: ')\n",
    "print(\"1-Star reviews:\",len(df_1_star))\n",
    "print(\"2-Star reviews:\",len(df_2_star))\n",
    "print(\"3-Star reviews:\",len(df_3_star))\n",
    "print(\"4-Star reviews:\",len(df_4_star))\n",
    "print(\"5-Star reviews:\",len(df_5_star))\n",
    "print('\\n')\n",
    "\n",
    "# Combine all of the dataframes\n",
    "frames = [df_1_star,df_2_star,df_3_star,df_4_star,df_5_star]\n",
    "\n",
    "df_balanced = pd.concat(frames)\n",
    "\n",
    "# Shuffle the data frame to randomize everything\n",
    "df_balanced = df_balanced.sample(frac=1)\n",
    "df_balanced.index = range(0,df_balanced.shape[0]) # Relabel the indices\n",
    "df_balanced.head()\n",
    "\n",
    "print('The total number of reviews: ', len(df_balanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the preprocessing functions, apply them to the balanced data set, and store the cleaned up documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/javier/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/javier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/javier/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/12105 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12105/12105 [03:46<00:00, 53.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from process_text import clean_up_text \n",
    "\n",
    "\n",
    "# Let us now generate a subset of the data to create a vocabulary with\n",
    "n_sample = len(df_balanced)\n",
    "dict_sample = []\n",
    "\n",
    "# Now we preprocess the entire balanced data set\n",
    "for i in tqdm(range(0,n_sample)):\n",
    "    sentence = str(df_balanced[\"review\"][i])\n",
    "    #print(sentence)\n",
    "    dict_sample.append(clean_up_text(sentence))\n",
    "\n",
    "# Convert to numpy array\n",
    "dict_sample = np.asarray(dict_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using preprocessed text, custom stop words, applying TFIDF transform build a vocabulary and generate a vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 12105 X 2880 TF-IDF-normalized document-term matrix\n",
      "\n",
      "01. nan (1477.05)\n",
      "02. not (483.40)\n",
      "03. get (313.45)\n",
      "04. work (291.72)\n",
      "05. like (283.83)\n",
      "06. printer (279.66)\n",
      "07. good (252.83)\n",
      "08. can (242.46)\n",
      "09. go (239.06)\n",
      "10. product (225.25)\n",
      "11. time (214.87)\n",
      "12. print (213.22)\n",
      "13. great (211.87)\n",
      "14. no (208.41)\n",
      "15. paper (204.17)\n",
      "16. dont (200.01)\n",
      "17. buy (197.94)\n",
      "18. ink (194.55)\n",
      "19. need (188.68)\n",
      "20. look (184.50)\n"
     ]
    }
   ],
   "source": [
    "# we can pass in the same preprocessing parameters\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator\n",
    "\n",
    "custom_stop_words = []\n",
    "with open( \"stopwords.txt\", \"r\" ) as fin:\n",
    "    for line in fin.readlines():\n",
    "        custom_stop_words.append( line.strip() )\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# Here we vectorize the dictionary sample\n",
    "vectorizer = TfidfVectorizer(stop_words = custom_stop_words,min_df = 20)\n",
    "A = vectorizer.fit_transform(dict_sample)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (A.shape[0], A.shape[1]) )\n",
    "print(\"\")\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# extract the resulting vocabulary\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "def rank_terms( A, terms ):\n",
    "    # get the sums over each column\n",
    "    sums = A.sum(axis=0)\n",
    "    # map weights to the terms\n",
    "    weights = {}\n",
    "    for col, term in enumerate(terms):\n",
    "        weights[term] = sums[0,col]\n",
    "    # rank the terms by their weight over all documents\n",
    "    return sorted(weights.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "ranking = rank_terms( A, terms )\n",
    "for i, pair in enumerate( ranking[0:20] ):\n",
    "    print( \"%02d. %s (%.2f)\" % ( i+1, pair[0], pair[1] ) )\n",
    "    \n",
    "# Write the vocabulary to a file\n",
    "f = open(\"vocabulary.txt\", \"w\")\n",
    "for i, pair in enumerate( ranking):\n",
    "    f.write( \"%02d. %s (%.2f) \\n\" % ( i+1, pair[0], pair[1] ) )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take the five-start ratings and condense into two outputs; +1 or -1, where +1 is the positive review, and -1 is a negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12105/12105 [00:01<00:00, 10129.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Dataset:  9684\n",
      "X- Feature matrix:  (9684, 2880)\n",
      "X_train:  7747\n",
      "X_test:  1937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Now we must transform our original review data into a feature Matrix\n",
    "y = df_balanced[\"rating\"]\n",
    "\n",
    "# Now we must transform our original review data into a feature Matrix\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# converting the data frame into a feature matrix\n",
    "for i in tqdm(range(0,df_balanced.shape[0])):\n",
    "    \n",
    "    r = df_balanced[\"rating\"][i]\n",
    "    \n",
    "    if(r==1 or r==2):\n",
    "        yi =-1\n",
    "        X.append(df_balanced[\"review\"][i])\n",
    "        y.append(yi)\n",
    "    elif(r==5 or r==4):\n",
    "        yi=1\n",
    "        X.append(df_balanced[\"review\"][i])\n",
    "        y.append(yi)\n",
    "print('Balanced Dataset: ',len(y))        \n",
    "\n",
    "## We store the remaining data \n",
    "df_final = df_balanced.drop(df_balanced[(df_balanced.rating == 2) | (df_balanced.rating == 3)| (df_balanced.rating== 4)].index)\n",
    "\n",
    "\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "# Converts the Document matrix consisting of strings into arrays according to the dictionary that we built previously\n",
    "X= vectorizer.transform(X)\n",
    "\n",
    "print('X- Feature matrix: ', X.shape)\n",
    "\n",
    "# Now we split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, random_state=random_state)\n",
    "\n",
    "print('X_train: ', len(y_train))\n",
    "print('X_test: ', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and test different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "\n",
      "Train data set size:  7747 \n",
      "\n",
      "Test data set size:  1937 \n",
      "\n",
      "================================================================\n",
      "\n",
      "Baseline Model: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.49      0.51      0.50       954\n",
      "          1       0.51      0.49      0.50       983\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1937\n",
      "\n",
      "================================================================\n",
      "\n",
      "Naive Bayes: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.80      0.84      0.82       954\n",
      "          1       0.84      0.79      0.81       983\n",
      "\n",
      "avg / total       0.82      0.82      0.82      1937\n",
      "\n",
      "================================================================\n",
      "\n",
      "Desicion Tree: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.74      0.73      0.73       954\n",
      "          1       0.74      0.75      0.75       983\n",
      "\n",
      "avg / total       0.74      0.74      0.74      1937\n",
      "\n",
      "================================================================\n",
      "\n",
      "Random Forests: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.74      0.81      0.77       954\n",
      "          1       0.79      0.73      0.76       983\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1937\n",
      "\n",
      "================================================================\n",
      "\n",
      "Logistic Regression: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.82      0.88      0.85       954\n",
      "          1       0.87      0.82      0.84       983\n",
      "\n",
      "avg / total       0.85      0.85      0.85      1937\n",
      "\n",
      "================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy.random import randint\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Now we train different models\n",
    "nb = MultinomialNB()\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(max_depth=20, random_state=0)\n",
    "lr = LogisticRegression(multi_class='multinomial',solver='newton-cg')\n",
    "\n",
    "# Fit the different machine learning models\n",
    "nb.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# The baseline model generates (-1 or 1) randomly\n",
    "preds_bl = np.ones(len(y_test))-2*randint(0,2,len(y_test))\n",
    "preds_nb = nb.predict(X_test)\n",
    "preds_dt = dt.predict(X_test)\n",
    "preds_rf = rf.predict(X_test)\n",
    "preds_lr = lr.predict(X_test)\n",
    "\n",
    "print('================================================================\\n')\n",
    "print('Train data set size: ', len(y_train),'\\n')\n",
    "print('Test data set size: ', len(y_test),'\\n')\n",
    "print('================================================================\\n')\n",
    "print(\"Baseline Model: \\n\",classification_report(y_test,preds_bl))\n",
    "print('================================================================\\n')\n",
    "print(\"Naive Bayes: \\n\" ,classification_report(y_test,preds_nb))\n",
    "print('================================================================\\n')\n",
    "print(\"Desicion Tree: \\n\",classification_report(y_test,preds_dt))\n",
    "print('================================================================\\n')\n",
    "print(\"Random Forests: \\n\",classification_report(y_test,preds_rf))\n",
    "print('================================================================\\n')\n",
    "print(\"Logistic Regression: \\n\",classification_report(y_test,preds_lr))\n",
    "print('================================================================\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we save the models and the text used to generate the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['articles-raw.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# Here we save the model\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "filename1 = 'lr_sentiment_model.sav'\n",
    "filename2 = 'nb_sentiment_model.sav'\n",
    "pickle.dump(lr, open(filename1, 'wb'))\n",
    "pickle.dump(nb, open(filename2, 'wb'))\n",
    "joblib.dump((X,terms,dict_sample), \"articles-raw.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model, test the models on some example text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best car is here\n",
      "  (0, 357)\t0.729356165256\n",
      "  (0, 234)\t0.684134185817\n",
      "\n",
      "lr:  [1] 0.788777675056\n",
      "nb:  [1] 0.620906800591\n",
      "TxtBlob:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Load the models and use them to make a prediction\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# The names of the files containing the weights of the model\n",
    "filename1 = 'lr_sentiment_model.sav'\n",
    "filename2 = 'nb_sentiment_model.sav'\n",
    "\n",
    "# Now we load in the trained models\n",
    "loaded_lr = pickle.load(open(filename1, 'rb'))\n",
    "loaded_nb = pickle.load(open(filename2, 'rb'))\n",
    "\n",
    "sample_text = 'The best car is here'\n",
    "s = str(clean_up_text(sample_text))\n",
    "\n",
    "print(sample_text)\n",
    "\n",
    "# Transform the text\n",
    "X0 = vectorizer.transform([s])\n",
    "print(X0)\n",
    "preds_nb = loaded_nb.predict(X0)\n",
    "preds_lr = loaded_lr.predict(X0)\n",
    "\n",
    "prob_nb = np.max(loaded_nb.predict_proba(X0))\n",
    "prob_lr = np.max(loaded_lr.predict_proba(X0))\n",
    "\n",
    "print(\"\")\n",
    "print('lr: ',preds_lr,prob_lr)\n",
    "print('nb: ',preds_nb,prob_nb)\n",
    "print('TxtBlob: ', TextBlob(s).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
