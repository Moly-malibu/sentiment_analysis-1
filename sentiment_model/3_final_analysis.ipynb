{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook produces the sentiment bar charts \n",
    "\n",
    "\n",
    "This notebook will perform the following steps:\n",
    "\n",
    "1. Read in and process tweets\n",
    "2. convert tweets into feature vectors\n",
    "3. Converted tweets are fed into classifiers which then compute the sentiment of the tweets with probabilities\n",
    "4. A threshold is selected, and tweets with a confidence below this threshold are classified as Neutral. Above this threshold, the sentiments are classified into positive or negative tweets.\n",
    "5. All tweets are processed and the final graphs are generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/javier/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/javier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/javier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os.path\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from process_text import clean_up_text\n",
    "from process_text import preprocess\n",
    "from process_text import string_cohesion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "car_company = \"Ford\"\n",
    "\n",
    "conn = sqlite3.connect('keyword_based_database.db')\n",
    "\n",
    "q_ford='''\n",
    "SELECT * FROM tweet\n",
    "WHERE LOWER(tweet_text) LIKE '%ford%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%harrison%'  \n",
    "AND LOWER(tweet_text) NOT LIKE '%doug%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%tom%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%henry%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%government%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%goverment%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%tax%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%toronto%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%democrats%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%ontario%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%fordnation%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%premier%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%melissa%'\n",
    "'''\n",
    "\n",
    "q_toyota='''\n",
    "SELECT * FROM tweet\n",
    "WHERE LOWER(tweet_text) LIKE '%toyota%'  \n",
    "'''\n",
    "\n",
    "\n",
    "q_mercedes='''\n",
    "SELECT * FROM tweet\n",
    "WHERE LOWER(tweet_text) LIKE '%mercedes%'\n",
    "'''\n",
    "\n",
    "q_bmw='''\n",
    "SELECT * FROM tweet\n",
    "WHERE LOWER(tweet_text) LIKE '%bmw%'  \n",
    "AND LOWER(tweet_text) NOT LIKE '%police%'\n",
    "LIMIT 10000;\n",
    "'''\n",
    "\n",
    "q_porche='''\n",
    "SELECT * FROM tweet\n",
    "WHERE LOWER(tweet_text) LIKE '%porsche%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%davido%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%nuamah%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%actor%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%police%'\n",
    "'''\n",
    "\n",
    "q_tesla='''\n",
    "SELECT * FROM tweet\n",
    "WHERE LOWER(tweet_text) LIKE '%tesla%'\n",
    "AND LOWER(tweet_text) LIKE '%model%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%elon%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%musk%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%solar roof%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%stock%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%production rate%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%money model%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%industry%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%net loss%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%loss%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%profit%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%atari%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%financial%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%earn%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%quarterly%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%record%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%market%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%outsell%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%produce%'\n",
    "AND LOWER(tweet_text) NOT LIKE 'second-quarter'\n",
    "AND LOWER(tweet_text) NOT LIKE 'invest'\n",
    "'''\n",
    "\n",
    "q_tesla='''\n",
    "SELECT * FROM tweet\n",
    "WHERE LOWER(tweet_text) LIKE '%tesla%'\n",
    "AND LOWER(tweet_text) NOT LIKE '%solar roof%'\n",
    "AND created_at LIKE '%Aug 13%'\n",
    "'''\n",
    "\n",
    "if(car_company==\"Toyota\"):\n",
    "    q = q_toyota\n",
    "elif(car_company==\"Ford\"):\n",
    "    q= q_ford\n",
    "elif(car_company==\"Mercedes\"):\n",
    "    q=q_mercedes\n",
    "elif(car_company==\"BMW\"):\n",
    "    q=q_bmw\n",
    "elif(car_company==\"Porche\"):\n",
    "    q=q_porche\n",
    "elif(car_company==\"Tesla\"):\n",
    "    q=q_tesla\n",
    "\n",
    "\n",
    "df = pd.read_sql_query(q,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some simple data exploration on the filtered tweets, histograms of the tweet lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some simple Data exploration\n",
    "\n",
    "N_raw_tweets = len(df)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print('Column names: \\n', list(df.columns.values),\"\\n\")\n",
    "print('The number of tweets: ', N_raw_tweets,'\\n')\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Show the First ten items in the tweet database\n",
    "print(\"Sample Tweets: \\n\")\n",
    "print(df['tweet_text'][0:100])\n",
    "\n",
    "\n",
    "\n",
    "# A small data frame to plot the length of the tweets and plot a histogram\n",
    "df_len= df['tweet_text'].apply(len)\n",
    "plt.hist(df_len,bins=20,density=True)\n",
    "plt.ylabel('Frequency',size=15)\n",
    "plt.xlabel('Tweet Length',size=15)\n",
    "plt.savefig('tweet_length_histogram_raw.pdf',bboxes='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Having loaded in the tweets we now process each tweet text and generate a histogram of the length of the processed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the 'Clean-up-text function to all tweets in the data frame'\n",
    "df['tweet_text'] = df['tweet_text'].apply(clean_up_text)\n",
    "\n",
    "\n",
    "# Remove duplicate tweets and reset the index\n",
    "df.drop_duplicates(subset='tweet_text', keep='first', inplace=True)\n",
    "df=df.reset_index(drop=True)\n",
    "\n",
    "N_processed_tweets = len(df)\n",
    "\n",
    "print('\\n')\n",
    "print('=============================================================')\n",
    "print('The number of tweets after removing duplicates: ', N_processed_tweets ,'\\n')\n",
    "print('Fraction removed: ', round(1.0-(N_processed_tweets/float(N_raw_tweets)),3) )\n",
    "print('=============================================================')\n",
    "\n",
    "# A small data frame to plot the length of the tweets\n",
    "df_len= df['tweet_text'].apply(len)\n",
    "plt.hist(df_len,bins=20,density=True)\n",
    "plt.ylabel('Frequency',size=15)\n",
    "plt.xlabel('Tweet Length',size=15)\n",
    "plt.savefig('tweet_length_histogram_processed.pdf',bboxes='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from process_text import clean_up_text\n",
    "from process_text import string_cohesion\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "# Import the custom stop words\n",
    "custom_stop_words = []\n",
    "with open( \"stopwords.txt\", \"r\" ) as fin:\n",
    "    for line in fin.readlines():\n",
    "        custom_stop_words.append( line.strip() )\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "# Import the vocabulary and generate the vectorizer tranformer\n",
    "#--------------------------------------------------------------\n",
    "(A,terms,dict_sample) = joblib.load( \"/home/javier/Documents/sentiment_analysis/sentiment_model/articles-raw.pkl\" )\n",
    "print( \"Loaded %d X %d document-term matrix\" % (A.shape[0], A.shape[1]) )\n",
    "print('number of terms: ',len(terms))#print(dic_sample[0:10])\n",
    "print('Dictionary: ',len(dict_sample))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = custom_stop_words,min_df = 20)\n",
    "A = vectorizer.fit_transform(dict_sample)\n",
    "print('A: ', A.shape)\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "# We load the trained models\n",
    "#--------------------------------------------------------------\n",
    "# The names of the files containing the weights of the model\n",
    "filename1 = 'lr_sentiment_model.sav'\n",
    "filename2 = 'nb_sentiment_model.sav'\n",
    "\n",
    "# Now we load in the trained models\n",
    "loaded_lr = pickle.load(open(filename1, 'rb'))\n",
    "loaded_nb = pickle.load(open(filename2, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will define the final sentiment model\n",
    "\n",
    "\n",
    "def sentiment_model(text,threshold):\n",
    "    \n",
    "    s = str(clean_up_text(text))\n",
    "    \n",
    "    X0 = vectorizer.transform([s])\n",
    "    \n",
    "    preds_nb = loaded_nb.predict(X0)\n",
    "    preds_lr = loaded_lr.predict(X0)\n",
    "    preds_blob =  TextBlob(text).sentiment.polarity\n",
    "    \n",
    "    if(preds_blob>=0.0):\n",
    "        preds_blob = +1\n",
    "    else:\n",
    "        preds_blob = -1\n",
    "        \n",
    "    indx_nb= int((preds_nb[0]+1)/2)\n",
    "    indx_lr= int((preds_lr[0]+1)/2)\n",
    "    prob_nb = abs((loaded_nb.predict_proba(X0)[0][indx_nb]-.50)*2.0*int(preds_nb[0]))\n",
    "    prob_lr = abs((loaded_lr.predict_proba(X0)[0][indx_lr]-0.50)*2.0*int(preds_lr[0]))\n",
    "    prob_blob = abs(TextBlob(text).sentiment.polarity) # [-1,1]\n",
    "    \n",
    "    \n",
    "    if(prob_lr<threshold):\n",
    "        preds_lr=[0.0]\n",
    "        \n",
    "    if(prob_nb<threshold):\n",
    "        preds_nb=[0.0]\n",
    "        \n",
    "    if(prob_blob<threshold):\n",
    "        preds_blob =0.0\n",
    "        \n",
    "        \n",
    "    # choose the most likely model\n",
    "    predictions = [preds_nb[0],preds_lr[0],preds_blob]\n",
    "    predict_prob = [prob_nb,prob_lr,prob_blob]\n",
    "    \n",
    "    pred = predictions[np.argmax(predict_prob)]\n",
    "    prob = np.max(predict_prob)\n",
    "    \n",
    "    \n",
    "    return pred,prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "thresholds = [0.1,0.2,0.3,0.4]\n",
    "sentiment_pred = []\n",
    "sentiment_prob = []\n",
    "ratio = []\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    \n",
    "    sentiment_pred = []\n",
    "    sentiment_prob = []\n",
    "    \n",
    "    for k in range(0,len(df)):\n",
    "        sample_text = df['tweet_text'][k]\n",
    "        pred,prob = sentiment_model(sample_text,threshold)\n",
    "        sentiment_pred.append(pred)\n",
    "        sentiment_prob.append(prob)\n",
    "        \n",
    "    m_count= collections.Counter(sentiment_pred)\n",
    "    ratio.append(m_count[-1]/m_count[1])\n",
    "    \n",
    "plt.plot(thresholds,ratio,\"-o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "\n",
    "\n",
    "sentiment_pred = []\n",
    "sentiment_prob = []\n",
    "    \n",
    "for k in range(0,len(df)):\n",
    "    sample_text = df['tweet_text'][k]\n",
    "    pred,prob = sentiment_model(sample_text,threshold)\n",
    "    sentiment_pred.append(pred)\n",
    "    sentiment_prob.append(prob)\n",
    "\n",
    "plt.hist(sentiment_pred)\n",
    "plt.xlabel(\"Sentiments\",size=15)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.hist(sentiment_prob,bins=25)\n",
    "plt.xlabel(\"Sentiment Probability\",size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Having selected the appropriate threshold for neutral comments, we compute the total number of positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import collections, numpy\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = 10, 9\n",
    "matplotlib.rcParams['font.size'] = 15\n",
    "\n",
    "# Count the various sentiments\n",
    "m_count= collections.Counter(sentiment_pred)\n",
    "\n",
    "size_model=[m_count[0],m_count[1],m_count[-1]]\n",
    "\n",
    "# Compute the total numbers\n",
    "total_number = np.sum(size_model)\n",
    "\n",
    "# Compute the Percentages\n",
    "percents = (np.asarray([m_count[0],m_count[1],m_count[-1]])/total_number)*100.0\n",
    "\n",
    "print(\"Total Sample: \",total_number)\n",
    "print(\"Neg/Pos ratio: \", m_count[-1]/m_count[1])\n",
    " \n",
    "# Create a circle for the center of the plot\n",
    "my_circle=plt.Circle( (0,0), 0.6, color='white')\n",
    "\n",
    "\n",
    "# create data\n",
    "names='Neutral: '+str(int(percents[0]))+\"%\",'Positive: '+str(int(percents[1]))+\"%\", 'Negative: '+str(int(percents[2]))+\"%\"\n",
    "\n",
    "# Give color names\n",
    "plt.title(car_company,size=30)\n",
    "plt.pie(size_model, labels=names, colors=['lightgrey','green','red'])\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.savefig(car_company+\"_N_\"+str(total_number)+\".pdf\",bboxes=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
